# 0.前言

去年10月、11月的时候参加了DataFountain的面向数据安全治理的数据内容智能发现与分级分类比赛https://www.datafountain.cn/competitions/471，最终获得了A榜第7、B榜第10的成绩。着这里记录一下此次比赛历程。

。

# 1.赛题背景

随着企业信息化水平的不断提高，数据共享与开放对企业发展的作用日益凸显，数据已成为重要生产要素之一，企业在产业与服务、营销支持、业务运营、风险管控、信息披露和分析决策等经营管理活动中涉及到大量的业务数据，其中可能会包含企业的商业秘密、工作秘密，以及员工的隐私信息等，若因为使用不当，造成数据泄露，则有可能造成巨大的经济损失，或在社会、法律、信用、品牌上对企业造成严重的不良影响。同时，在合规要求层面，围绕数据安全，国家近年密集颁布《网络安全法》、《民法典》、《数据安全法》（征求意见稿）、《个人信息保护法》（征求意见稿）等，从国家法律层面强调对关键基础设施、各类APP应用中的敏感数据保护要求。而为了有效、规范保护企业敏感数据，其首要问题是对数据进行分级分类，以识别敏感数据，从而进一步围绕保护对象的全生命周期进行开放、动态的数据安全治理，解决数据开放共享与数据隐私保护的矛盾与统一。
现有的敏感数据识别与分级分类已广泛采用基于自然语言处理的语义识别技术，但会存在以下问题：
1.需要有大批量、高质量的标注数据，花费大量的人力和时间，建设成本高。
2.泛化能力不足，对新业务数据的适应能力弱，敏感数据的误报率和漏报率高。
3.不能进行自我优化、自我学习，需要业务和技术领域专家共同进行人工干预，建设难度大。

# 2.赛题任务

识别样本中的敏感数据，构建基于敏感数据本体的分级分类模型，判断数据所属的类别以及级别。
1.利用远程监督技术，基于小样本构建文档分类分级样本库。
2.结合当下先进的深度学习和机器学习技术，利用已构建的样本库，提取文本语义特征，构建泛化能力强且能自我学习的文档分类分级模型。

# 3.数据简介与说明

（1）已标注数据：共7000篇文档，类别包含7类，分别为：财经、房产、家居、教育、科技、时尚、时政，每一类包含1000篇文档。
（2）未标注数据：共33000篇文档。
（3）分类分级测试数据：共20000篇文档，包含10个类别:财经、房产、家居、教育、科技、时尚、时政、游戏、娱乐、体育。

本次大赛提供两份数据，已标注数据labeled_data.csv，未标注数据 unlabeled_data.csv，分类分级测试数据test_data.csv。
（1）已标注数据 labeled_data.csv

|  字段信息   |  类型  |     描述     |
| :---------: | :----: | :----------: |
|     id      | String |    数据ID    |
| class_label | String | 文本所属类别 |
|   content   | String |   文本内容   |

（2）未标注数据unlabeled_data.csv

| 字段信息 |  类型  |   描述   |
| :------: | :----: | :------: |
|    id    | String |  数据ID  |
| content  | String | 文本内容 |

（3）分类分级测试数据 test_data.csv

| 字段信息 |  类型  |   描述   |
| :------: | :----: | :------: |
|    id    | String |  数据ID  |
| content  | String | 文本内容 |

（4）分级信息
假设文档类别与文档级别有如下对应关系：

|     文档类别     | 文档级别 |
| :--------------: | :------: |
|    财经、时政    |  高风险  |
|    房产、科技    |  中风险  |
| 教育、时尚、游戏 |  低风险  |
| 家居、体育、娱乐 |  可公开  |

**本次比赛不允许使用外部数据集~**

可以简单的理解为，此次比赛是一个文本分类问题；但是给的训练集有7类数据，测试集有10类数据，需要事先在未标注数据（共33000篇文档）中采用无监督的方法选取标签为另外3类的部分数据，加入到训练集中训练。

# 4.无监督文本分类

由于本次提供的训练集共7000篇文档，类别包含7类；首先需要在unlabled数据集中采用无监督的方法将另外3类（游戏、娱乐、体育）分出来；

本文采用了LDA文本聚类算法，首先通过该算法将未标注33000篇文档聚为10类，然后从类别为游戏、娱乐、体育的数据中各随机出1000条，与原始数据组成10000条的训练集。

# 5.方法介绍

1. 句子长度选取

   由于本次数据文本为新闻内容，长度较长，而bert对于文本长度有所限制，考虑到新闻的关键信息常常集中在文首或文尾，所以采取截断的方式选取超长文本。

   ```python
   if len(text) > 512:
       text = text[:256] + text[-256:]
   ```

2. 模型尝试

   chinese_roberta_wwm_large_ext_pytorch模型跑分最高（模型较大，文本较长，吃GPU）。

3. drouout选取

   在训练过程中，由于Bert后接了dropout层。为了加快模型的训练，我们使用multi-sample-dropout技术。通过对Bert后的dropout层进行多次sample，并对其多次输出的loss进行平均，增加了dropout层的稳定性，同时使得Bert后面的全连接层相较于前面的Bert部分能够得到更多的训练。

   dropout设置为0.9时效果最佳。

4. 文本对抗（https://zhuanlan.zhihu.com/p/91269728）

   尝试了PGD和FGM，PGD最佳。

5. **loss权重**

   通过比赛时发现，测试集中的类别基本平衡，所以根据分类结果，来调试loss的权重，使得分类结果保持基本平衡，这是本次比赛提分的一个关键。

   ```python
   loss_fn = nn.CrossEntropyLoss(weight=torch.from_numpy(np.array(args.weight_list)).float())
   ```

6. 模型融合

   通过对bert模型最后几层输出采用不同的方式组合构建多种模型，进行简单的投票融合。

   例如BertLastFourCls、BertLastFourEmbedding、BertRCNN、BertLastFourClsPooler、BertLastTwoEmbeddingsPooler等等，具体可参考代码。

   

最终A榜成绩0.91007184 ,B榜0.91033015。

# 6.使用的其他trick

1. EDA文本增强技术（https://github.com/jasonwei20/eda_nlp）（无提升）。

2. 采用半监督的思想，对测试集预测，并将预测结果与原有数据一起训练（无提升）。

3. F1值优化(效果不稳定)

4. 使用THUCnews进行模型预训练（后因主办方不允许使用外部数据放弃），单纯使用部分THUCnews做为训练集训练，对测试集进行预测得分只有0.5+。

   预训练参考（https://github.com/zhusleep/pytorch_chinese_lm_pretrain）

   



# 7.前排大佬解决方案

代更新~